
        
        
        [{"content":"We analyze 1 the theoretical properties of both the Discrete Fourier Transform (DFT) and the optimized Fast Fourier Transform (FFT), and estimate their algorithmic complexity.\nAll code available on Github. Slides available here.\nDiscrete Fourier Transform Theory The DFT converts a finite-length time-domain signal into its frequency-domain representation. For an input signal $x[n]$ of length $N$, where $0 \\le n \\le N-1$, the DFT is defined as:\n$$ X[k] = \\sum_{n=0}^{N-1} x[n]\\, W_N^{kn}, \\quad 0 \\le k \\le N-1 \\label{eq:DFT} $$ where $W_N=e^{-j\\frac{2\\pi}{N}}$ is the $N$-th principal root of unity, and $X[k]$ is component at frequency $kf_s/N$ with sampling frequency $f_s$. We can rewrite $X[k]$ as an inner product:\n$$ \\begin{align} X[k] \u0026= \\left[ 1 \\quad e^{-j\\frac{2\\pi k}{N}} \\quad \\ldots \\quad e^{-j\\frac{2\\pi k}{N}(N-1)} \\right] \\begin{bmatrix} x[0] \\\\ x[1] \\\\ \\vdots \\\\ x[N-1] \\end{bmatrix} \\\\ \u0026= \\left[ 1 \\quad W_N^k \\quad \\ldots \\quad W_N^{(N-1)k} \\right] \\begin{bmatrix} x[0] \\\\ x[1] \\\\ \\vdots \\\\ x[N-1] \\end{bmatrix} \\end{align} $$ By varying $k$ from $0$ to $N-1$, we can collate all outputs ${X[k]}$ into a vector $\\mathbf{X}$, collate all inputs $x[n]$ into a vector $\\mathbf{x}$, to get:\n$$ \\mathbf{X}=\\mathbf{W}\\mathbf{x}, \\quad \\mathbf{W} = \\begin{bmatrix} 1 \u0026 1 \u0026 1 \u0026 \\cdots \u0026 1 \\\\ 1 \u0026 W_N \u0026 W_N^2 \u0026 \\cdots \u0026 W_N^{N-1} \\\\ 1 \u0026 W_N^2 \u0026 W_N^4 \u0026 \\cdots \u0026 W_N^{2(N-1)} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 1 \u0026 W_N^{N-1} \u0026 W_N^{2(N-1)} \u0026 \\cdots \u0026 W_N^{(N-1)(N-1)} \\end{bmatrix} $$ where $\\mathbf{W}$ is the DFT matrix.\nComplexity Pseudocode for DFT Direct implementation in MATLAB:\nfunction X = dft_loop(x) % Compute the Discrete Fourier Transform (DFT) of input vector x using naive loops % x : input signal (vector) % X : DFT of x % Convert input to a column vector for consistency x = x(:); N = length(x); % Number of samples in the signal % Pre-allocate the output vector for efficiency X = zeros(N, 1); % Loop over each frequency bin k (from 0 to N-1) for k = 0:N-1 sum_val = 0; % Loop over each time sample n (from 0 to N-1) for n = 0:N-1 % Compute and accumulate the contribution for the k-th frequency component sum_val = sum_val + x(n+1) * exp(-1j * 2 * pi * k * n / N); end % MATLAB uses one-based indexing, so assign to X(k+1) X(k+1) = sum_val; end end MATLAB code (vectorized):\nfunction X = dft_vectorized(x) % Compute the Discrete Fourier Transform (DFT) of input vector x using vectorized operations % x : input signal (vector) % X : DFT of x % Convert input to a column vector for consistency x = x(:); N = length(x); % Number of samples in the signal % Create index vectors: n as a row vector and k as a column vector n = 0:N-1; % Time indices (row vector) k = n\u0026#39;; % Frequency indices (column vector) % Construct the DFT matrix using the formula: exp(-1j*2*pi*k*n/N) W = exp(-1j * 2 * pi * k * n / N); % Multiply the DFT matrix with the signal to obtain the transform X = W * x; end Direct computation of $\\mathbf{X}$ requires $(N-1)^2$ complex multiplications and $N(N-1)$ complex additions. Since multiplication is more compute-intensive than addition, the asymptotic complexity is dominated by multiplication, giving $O(N^2)$.\nFast Fourier Transform Preliminaries We layout some properties of $W_N$ we\u0026rsquo;ll be using later\nProperty 1:\n$$ W_N^2 = W_{N/2} $$ Proof:\n$$ W_N^2 = e^{-j\\frac{2\\pi}{N}\\cdot 2} = e^{-j\\frac{2\\pi}{N/2}} = W_{N/2} $$ More generally, $W_N^{2nk} = W_{N/2}^{nk}$.\nProperty 2:\n$$ W_N^{k+\\frac{N}{2}} = -W_N^k $$ Proof:\n$$ W_N^{k+\\frac{N}{2}} = e^{-j\\frac{2\\pi}{N}\\left(k+\\frac{N}{2}\\right)} = e^{-j\\frac{2\\pi}{N}k} \\cdot e^{-j\\pi} = -W_N^k $$ Theory The radix-2 FFT algorithms work by dividing the DFT into 2 DFTs of length $N/2$ each, and iterating. We introduce the simplest variant, called the Decimation-In-Time (DIT) algorithm.\nConsider a $N$-point signal $x[n]$ of even length, indexed from $0$ to $N-1$. The derivation of the DIT radix-2 FFT begins by splitting the $x[n]$ into two parts \u0026mdash; one part for the even-indexed values $x[2n]$ and one part for the odd-indexed values $x[2n + 1]$. Define two $N/2$-point signals $x_{even}[n]$ and $x_{odd}[n]$ as $$x_{even}[n] = x[2n], \\quad x_{odd}[n] = x[2n + 1], \\quad 0 \\leq n \\leq N/2-1$$ The DFT can be written as\n$$ \\begin{aligned} X[k] \u0026= \\sum_{n=0 \\atop n \\text{ even}}^{N-1} x[n] W_N^{nk} + \\sum_{n=0 \\atop n \\text{ odd}}^{N-1} x[n] W_N^{nk}\\\\ \u0026= \\sum_{n=0}^{N/2-1} x[2n] W_N^{2nk} + \\sum_{n=0}^{N/2-1} x[2n + 1] W_N^{(2n+1)k}\\\\ \u0026= \\sum_{n=0}^{N/2-1} x_{even}[n] W_N^{2nk} + \\sum_{n=0}^{N/2-1} x_{odd}[n] W_N^{(2n+1)k} \\\\ \u0026= \\sum_{n=0}^{N/2-1} x_{even}[n] W_N^{2nk} + W_N^{k} \\cdot \\sum_{n=0}^{N/2-1} x_{odd}[n] W_N^{2nk}\\\\ \\end{aligned} $$ Noting that $W_N^{2}=W_{N/2}$ or more generally $W_N^{2nk}=W_{N/2}^{nk}$ , then\n$$ \\begin{aligned} X[k]\u0026= \\sum_{n=0}^{N/2-1} x_{even}[n] W_{N/2}^{nk} + W_N^{k} \\cdot \\sum_{n=0}^{N/2-1} x_{odd}[n] W_{N/2}^{nk} \\end{aligned} $$ Recognizing that the $\\frac{N}{2}$-point DFT of $x_{even}[n]$ and $x_{odd}[n]$ are given by:\n$$X_{even}[k] = \\text{DFT}_{\\frac{N}{2}}\\{x_{even}[n]\\} = \\sum_{n=0}^{N/2-1} x_{even}[n] W_{N/2}^{nk}$$ $$X_{odd}[k] = \\text{DFT}_{\\frac{N}{2}}\\{x_{odd}[n]\\} = \\sum_{n=0}^{N/2-1} x_{odd}[n] W_{N/2}^{nk}$$ we then obtain our core recursive equation:\n$$ X[k] = X_{even}[k] + W_N^{k} \\cdot X_{odd}[k] \\label{eq:fft_int} $$ Since $x_{even}[n]$ and $x_{odd}[n]$ are $N/2$-point signals with $0 \\leq n \\leq N/2-1$, their DFT are also only $N/2$-point signals. However, we require $0 \\le k \\le N-1$. We resolve this by noting their DFT coefficients are periodic with a period of $\\frac{N}{2}$:\n$$X_{even}[k] = X_{even}\\left[k + \\frac{N}{2}\\right], \\quad X_{odd}[k] = X_{odd}\\left[k + \\frac{N}{2}\\right]$$ This gives us\n$$ X[k] = \\begin{cases} X_{even}[k] + W_N^{k} \\cdot X_{odd}[k], \u0026 \\text{for } k = 0,1,\\ldots,\\frac{N}{2}-1 \\\\ X_{even}\\left[k-\\frac{N}{2}\\right] + W_N^{k} \\cdot X_{odd}\\left[k-\\frac{N}{2}\\right], \u0026 \\text{for } k = \\frac{N}{2},\\frac{N}{2}+1,\\ldots,N-1 \\end{cases} $$ Noting $W_N^{k +\\frac{N}{2}}=-W_{N}^{k}$, we write\n$$ X[k] = \\begin{cases} X_{even}[k] + W_N^{k} \\cdot X_{odd}[k], \u0026 \\text{for } k = 0,1,\\ldots,\\frac{N}{2}-1 \\\\ X_{even}\\left[k-\\frac{N}{2}\\right] - W_N^{k-\\frac{N}{2}} \\cdot X_{odd}\\left[k-\\frac{N}{2}\\right], \u0026 \\text{for } k = \\frac{N}{2},\\frac{N}{2}+1,\\ldots,N-1 \\end{cases} $$ finally giving us\n$$ \\begin{aligned} X[k] \u0026= X_{even}[k] + W_N^{k} \\cdot X_{odd}[k] \\quad \\text{for } 0 \\leq k \\leq \\frac{N}{2} - 1 \\\\ X[k + N/2] \u0026= X_{even}[k] - W_N^{k} \\cdot X_{odd}[k] \\quad \\text{for } 0 \\leq k \\leq \\frac{N}{2} - 1 \\label{eq:fft_final} \\end{aligned} $$ The multipliers $W_N^k$ are known as twiddle factors. The first computation with $+W_N^k$ give us the first half of the full DFT vector $\\mathbf{X}$, while the second computation with $-W_N^k$ give us the second half of $\\mathbf{X}$.\nComplexity Pseudocode for FFT MATLAB code for FFT:\nfunction X = fft_vectorized(x) % computes the radix-2 FFT recursively using vectorized operations. % If the length of x is not a power of 2, it is zero-padded to the next power of 2, added at the end x = x(:); % Ensure x is a column vector N = length(x); % If N is not a power of 2, zero-pad x to the next power of 2 M = 2^nextpow2(N); % nextpow2 returns the exponent so that 2^exponent \u0026gt;= N if M ~= N %add zeros to the end x = [x; zeros(M - N, 1)]; N = M; % Update N to the new length end % Base case: if the input length is 1, return x if N == 1 X = x; return; end % Recursively compute FFT for even and odd indices X_even = fft_vectorized(x(1:2:end)); %select all the even indices X_odd = fft_vectorized(x(2:2:end)); % selects all the odd indices % Compute twiddle factors (complex exponentials) in a vectorized manner % row array factor = exp(-1j * 2 * pi * (0:N/2-1).\u0026#39; / N); % Combine the FFTs of the even and odd parts using the butterfly operation X = [X_even + factor .* X_odd; X_even - factor .* X_odd]; end We can follow the above procedure to split an $N$-point DFT into two $\\frac{N}{2}$-point DFT, giving us the above algorithim. Let $A_c(N)$ and $M_c(N)$ denote respectively the number of complex additions and multiplications for computing the DFT of an $N$-point complex sequence $x[n]$. Let $N$ be a power of 2, $N = 2^k$. Then, we have $$A_c(N) = 2 A_c(N/2) + N \\quad , \\quad M_c(N) = 2 M_c(N/2) + \\frac{N}{2} - 1$$ as $N$ complex additions (addition of even and odd terms) and $\\frac{N}{2} - 1$ complex multiplications ($W_N^{k} \\cdot X_{odd}[k]$) are required to put the two $N/2$-point DFTs together. Note that a 2-point DFT is simply a sum and difference\n$$X[0] = x[0] + x[1] , \\quad X[1] = x[0] - x[1]$$ Hence, the starting conditions are $A_c(2) = 2$ and $M_c(2) = 0$. Solving the recursive equation yields $$A_c(N) = N \\log_2 N \\quad , \\quad M_c(N) = \\frac{N}{2} \\log_2 N - N + 1$$ Our overall complexity is $O(N\\log N)$.\nExperiments Complexity Execution time versus signal length $N$ for various implementations of DFT and FFT Algorithm O(N) O(N²) O(N³) O(log N) O(N log N) my-DFT 0.9208 0.9999 0.9864 0.4167 0.9417 my-FFT 0.9830 0.9657 0.9161 0.5634 0.9894 MATLAB-FFT 0.8491 0.7141 0.6526 0.8537 0.8287 To verify the theoretical complexity, we measured the execution time for computing the DFT over a range of signal lengths. For each signal length $N$, $N$ samples were taken from a 5Hz sine wave, and the DFT/FFT computation time recorded. We compare our implementation of DFT my-DFT, our implementation of FFT as my-FFT, and the MATLAB FFT implementation as MATLAB-FFT. You can view the measured execution times above on a log-log plot. The FFT results clearly exhibit an $O(N\\log N)$ scaling, while the DFT scales as $O(N^2)$. These experimental results confirm the significant computational advantage of using the FFT for large-scale problems. We also note the much more efficient implementation of MATLAB FFT, which uses the FFTW 2 package. The table above also shows the how well different models fit to data. Both FFT implementations fit well to $O(N)$ and $O(N\\log N)$ models.\nNumerical Error Reconstruction error versus signal length $N$ for various implementations of DFT and FFT We evaluate numerical accuracy of above algorithims by measuring reconstruction error of the reconstructed signal $\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}|x_i - \\hat{x}_i|^2}$ where $x_i$ is the original signal and $\\hat{x}_i$ is the reconstructed signal (using the MATLAB Inverse DFT function) Despite all errors falling within the $10^{-12}$ range, indicating high overall accuracy, the custom DFT implementation exhibits exponentially growing error with increasing sequence length. In contrast, both FFT implementations maintain consistently minimal error across all tested sequence lengths. The superior numerical stability of FFT algorithms is possibly due to its lower operation count (lower algorithmic complexity). Floating-point rounding errors accumulate more significantly in DFT.\nConclusion We study the theoretical motivations behind FFT. We verify that the computational complexity of the direct DFT implementation scales as $O(N^2)$, whereas our FFT implementation achieved the expected $O(N \\log N)$ complexity, offering a significant improvement in efficiency.\nReferences Golub, G. H., \u0026amp; Van Loan, C. F. (1996). Matrix computations (3rd ed.). Johns Hopkins University Press.\nStrang, G. (2007). Computational Science and Engineering. Wellesley-Cambridge Press. https://epubs.siam.org/doi/abs/10.1137/1.9780961408817\nRamalingam, C.S. Introduction to the Fast-Fourier Transform (FFT) Algorithm. https://www.ee.iitm.ac.in/~csr/teaching/pg_dsp/lecnotes/fft.pdf\nFast Fourier Transform (FFT). NYU Engineering. https://eeweb.engineering.nyu.edu/iselesni/EL713/zoom/fft\nThis post was created for educational purposes, so much of the analysis is taken directly from the sources quoted in the references\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis package is highly optimized to each machine and is written in low level C\u0026#160;\u0026#x21a9;\u0026#xfe0e;","href":"https://liuzihe02.github.io/posts/dft-fft/","kind":"page","lang":"en","lastmod":"2025-03-28T17:05:08Z","objectID":"edf0899851b8b46beb0b136c155139c4","publishDate":"2025-03-28T17:05:08Z","section":"posts","tags":[],"title":"The Discrete Fourier Transform and Fast Fourier Transform","type":"posts"},{"content":"In this post, I want to explore the key RL architecture and software optimizations DeepSeek made to create Deepseek-R1. I\u0026rsquo;ll provide a brief introduction to the RL setup and explain how GRPO makes traditional algorithims much more efficient. At the end, I\u0026rsquo;ll provide a brief overview of how DeepSeek-R1 was trained. Talk given at Cambridge AI Safety Hub Members Meeting in February, slides available here.\nPreliminaries Transformers Illustration of autoregressive model of transformers, taken from the ARENA curriculum Modern transformers are autoregressive models. They predict the next token (essetially a word) given all the tokens before it:\n$$p(x_1,\\ldots,x_N) = \\prod_{n=1}^N p(x_n|x_1,\\ldots,x_{n-1})$$ Transformer architecture overview, taken from the ARENA curriculum At a high level, input natural language are broken up into tokens, and converted to a numerical form. The output is then a probability distribution over the next possible token (from logits), and the next token is sampled from this distribution.\nReinforcement Learning We provide a brief overview of the RL setup.\nAn agent in state $s$ issues an action $a$ to the environment, and the environment replies with state, reward pairs $(s',r)$. We can then define a trajectory $\\tau$ as a sequence of states and actions: $s_0, a_0, r_1, a_1, r_2, s_2, a_2, r_3...$\nThe agent chooses actions using a policy $\\pi$ which can either be a deterministic function $a=\\pi(s)$ from states to actions, or more generally actions are sampled $a \\sim \\pi(\\cdot|s)$\nThe return at time $t$ is the sum of rewards obtained after time $t$ until the time $T$ when the terminal state is reached, discounted by how far into the future:\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} ...$$ The state value function $V_\\pi(s)$ is the expected return, if you start in state $s$ and always act according to policy $\\pi$:\n$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[G_t| s_t = s\\right]$$ We deal with these core quantities:\nThe action value function is the expected return if you start in state $s$, take an arbitrary action $a$ (which may not have come from the policy), and then afterwards forever act according to policy $\\pi$: $$Q_\\pi(s, a) = \\mathbb{E}_{\\pi} \\left[G_t| s_t = s, a_t = a\\right]$$ The advantage function describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to $\\pi(\\cdot|s)$, assuming you act according to $\\pi$ forever after:\n$$A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s)$$ Trust Region Policy Optimization (TRPO)\nLet $\\pi_\\theta$ denote a policy with parameters $\\theta$. We aim to maximize the expected return $J$ of policy $\\pi_\\theta$, where we take expectation over all possible trajectories $\\tau$ generated by following policy $\\pi_\\theta$\n$$J(\\pi_\\theta) = \\underset{\\tau \\sim \\pi_\\theta}{\\mathbb{E}}[G(\\tau)]$$ To achieve the optimum $\\pi_\\theta$ that maximizes J, we can iteratively update $\\pi_{\\theta_{k}}$ according to:\n$$\\theta_{k+1} = \\arg \\max_\\theta \\mathcal{L}(\\theta_k, \\theta) = \\arg \\max_\\theta \\underset{s,a\\sim\\pi_{\\theta_k}}{\\mathbb{E}} \\left[L(s, a, \\theta_k, \\theta)\\right], \\quad \\text{s.t. } \\bar{D}_{KL}(\\theta\\|\\theta_k) \\leq \\delta$$ where $\\mathcal{L}(\\theta_k, \\theta)$ is the surrogate advantage, a measure of how policy $\\pi_\\theta$ performs relative to the old policy $\\pi_{\\theta_k}$ using data from the old policy:\n$$L(s, a, \\theta_k, \\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)} A_{\\pi_{\\theta_k}}(s,a)$$ TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be.\nProximal Policy Optimization (PPO)\nProximal Policy Optimization updates policies via:\n$$\\theta_{k+1} = \\arg \\max_\\theta \\mathcal{L}(\\theta_k, \\theta) = \\arg \\max_\\theta \\underset{s,a\\sim\\pi_{\\theta_k}}{\\mathbb{E}} \\left[L(s, a, \\theta_k, \\theta)\\right],$$ typically taking multiple steps of SGD to maximize the objective. Here $L$ is given by:\n$$L(s, a, \\theta_k, \\theta) = \\min\\Bigg(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A_{\\pi_{\\theta_k}}(s,a), \\text{clip}\\left(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}, 1-\\epsilon, 1+\\epsilon\\right)A_{\\pi_{\\theta_k}}(s,a)\\Bigg)$$ PPO-Clip doesn\u0026rsquo;t have a KL-divergence term in the objective and doesn\u0026rsquo;t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\nGeneralized Advantage Estimation (GAE) is used in PPO to estimate the advantage function:\n$$A_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} \\\\ = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} + \\dots$$ where:\n$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ (TD error)\nthe reward $r_t$ is usually given by a trained (neural) reward model $\\gamma$ is the discount factor\n$\\lambda$ is the GAE parameter (tradeoff between bias and variance)\n$V(s)$ is the value function estimate, from value network\nOften a value network $V_\\psi$ needs to be trained alongside the policy model $\\pi_\\theta$ to estimate the value, often as big as the policy model itself General RL Training Setup Objective: $$J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right],$$ where $\\tau = \\{s_0, a_0, r_1, s_1, a_1, \\dots, s_T\\}$ is a trajectory. For LLMs, actions $a_t$ here are tokens!\nPolicy Gradient: $$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{s,a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\, Q^{\\pi_\\theta}(s,a)\\right],$$ or equivalently, using the advantage function $A^{\\pi_\\theta}(s,a)$: $$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{s,a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\, A^{\\pi_\\theta}(s,a)\\right].$$ Trajectory Collection: LLM interacts with the environment (reward model) to sample trajectories. For each timestep: $$s_t \\xrightarrow{\\pi_\\theta} a_t \\rightarrow r_{t+1},\\, s_{t+1}.$$ Return and Advantage Calculation: $$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}, \\quad A(s_t,a_t) = Q^{\\pi_\\theta}(s_t,a_t) - V^{\\pi_\\theta}(s_t).$$ Generalized Advantage Estimation (GAE) are used to compute $A(s_t,a_t)$ practically\nPolicy Update: $$\\theta_{k+1} = \\theta_k + \\alpha\\, \\nabla_\\theta J(\\pi_\\theta),$$ Using the computed advantages, update the policy parameters and value network by maximizing a surrogate objective (e.g., the PPO objective) to favor actions with higher expected returns. Repeat until agent\u0026rsquo;s performance converges.\nR1-Zero Group Relative Policy Optimization (GRPO) DeepSeek uses GRPO (created in-house) which foregoes the value network and estimates the advantages directly from group rewards instead.\nFor each question $q$, GRPO samples a group of outputs $\\{o_i\\}_{i=1}^G$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing:\n$$\\begin{aligned} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(Q|q)] \\\\ \u0026\\quad \\cdot \\frac{1}{G}\\sum_{i=1}^G\\left(\\min\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}A_i, \\text{clip}\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon\\right)A_i\\right) - \\beta\\mathbb{D}_{KL}(\\pi_\\theta\\|\\pi_{\\text{ref}})\\right) \\end{aligned}$$ $$\\mathbb{D}_{KL}(\\pi_\\theta\\|\\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - \\log\\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - 1$$ where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_i$ is the advantage 1\nGRPO Advantages\nAdvantages $A_i$ are computed using group rewards $\\{r_1,r_2,\\ldots,r_G\\}$:\n$$A_i = \\frac{r_i - \\text{mean}(\\{r_1,r_2,\\ldots,r_G\\})}{\\text{std}(\\{r_1,r_2,\\ldots,r_G\\})} $$ As the value network is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. GRPO eliminates value network completely, saving enourmous amounts of compute!\nReward Modelling\nR1-Zero uses a rule-based reward system consisting of:\nAccuracy rewards: The accuracy reward model evaluates whether the response is correct. For math problems with deterministic results, can easily check if right or wrong. Provider higher reward if right. For LeetCode problems, a compiler can be used to generate feedback based on predefined test cases Format rewards: Enforces the model to put its thinking process between \u0026lt;think\u0026gt; and \u0026lt;/think\u0026gt; tags. Usually for LLMs, a neural reward model (neural network used to estimate rewards) is used to provide rewards, as human preferences or requirements are difficult to define in a rules-based way. However, R1 does not use any neural reward model:\nNeural reward model may suffer from reward hacking\nRetraining the reward model needs additional training resources and it complicates the whole training pipeline\nPerformance\nThere\u0026rsquo;s no supervised fine tuning involved at all in R1-Zero, which is extremely impressive considering its performance. Majority voting bring performance on AIME benchmark from 71% to 86%.\nR1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation (increased Chain-Of-Thought length). Behaviours like backtracking and reflection naturally emerge, which is absolutely incredible\u0026hellip;\nProblems\nR1-Zero exhibits poor readability of its chain of thoughts, where language mixing often occurs. I suspect this is probably some form of reward over-optimization, as it is more efficient to reason in its own language and achieve higher reward.\nWe know different languages have different information density. This can be thought of as the \u0026ldquo;efficiency\u0026rdquo; of language, given a constraint on its length. Suppose an LLM had a length constraint (the context window), and was incentivised to achieve a goal. Won\u0026rsquo;t you expect the LLM to naturally use more information dense languages? Or even more extreme - if it were intelligent enough - create its own language that is information dense enough to achieve its goals?\nI worry about this aspect quite abit, about future intelligent systems reasoning in its own language uninterpretable to humans. If you\u0026rsquo;ve ever seen Villeneuve\u0026rsquo;s incredible film Arrival, you would have seen the heptapods\u0026rsquo; non-linear language which isn\u0026rsquo;t formed by sequentially combining words linearly. If these systems start communicating in their own language and are given goals by humans, you can see how this could cause all sorts of various problems.\nR1 Training Pipeline Phase 1: Cold Start Construct and collect small amounts (thousands) of long CoT data to fine-tune V3 Using few-shot prompting on V3 with long CoT examples Direct prompting on V3 for detailed answers with reflection Gathering DeepSeek-R1-Zero outputs with human post-processing Key advantages: Readability: R1-Zero has unreadable responses. We define output format as |special_token|\u0026lt;reasoning_process\u0026gt;|special_token|\u0026lt;summary\u0026gt; so reasoning process (CoT) and summary of reasoning provided Potential: Carefully designed patterns with human priors show better performance vs R1-Zero Phase 2: Reasoning Reinforcement Learning After cold start fine-tuning, apply large-scale RL training similar to DeepSeek-R1-Zero:\nFocus on reasoning-intensive tasks: Coding, mathematics, science, logic reasoning Well-defined problems with clear solutions Language Consistency: CoT exhibits language mixing in multi-language prompts Introduce language consistency reward, measured as proportion of target language words in CoT Final reward function combines reasoning task accuracy, language consistency reward, and formatting reward to train until convergence.\nPhase 3: Rejection Sampling and Supervised Fine-Tuning After RL convergence, collect SFT data from checkpoint for further training. Data includes:\nReasoning Data (600k samples)\nRejection sampling from RL checkpoint Expanded dataset includes: Rule-based rewards Generative rewards via DeepSeek-V3 as LLM-Judge Quality filters: Remove mixed languages Filter long paragraphs Remove chaotic code blocks Non-Reasoning Data (200k samples)\nTypes: Writing Factual QA Self-cognition Translation Reuse SFT data for DeepSeek-V3 Final fine-tuning on DeepSeek-V3-Base: 2 epochs on combined 800k samples\nPhase 4: Diverse Reinforcement Learning Phase Secondary RL stage to align with human preferences while improving, helpfulnes, harmlessness, and reasoning:\nTraining Approach\nReasoning data: Same approach as DeepSeek-R1-Zero Rule-based rewards on Math, code, logical reasoning General data: Reward model as in DeepSeek V3 Use preference pairs Helpful Honest Harmless (HHH) Criteria\nHelpfulness: Focus on final summary in summary tag is useful and relevance Preserves reasoning process in the reasoning tag Harmlessness: Evaluate full response for potential biases or hamful content R1 Engineering Unlocks The DeepSeek team also did some insane software and hardware optimizations:\nMulti-Head Latent Attention (for efficient inference) Low-rank join compression for K,Q,V matrices to reduce key-value cache memory usage DeepSeekMoE with Auxiliary-Loss-Free Load Balancing (efficient training) 671B parameters, but only which 37B are activated for each token Multi-Token Prediction Objective Conclusion I think the core reason behind R1\u0026rsquo;s success was its sheer efficiency: getting rid of the value network via simple advantage estimation, using a simple reward model instead of a neural reward model, and all the hardware/software optimizations. The multi-phase training approach, while quite hacky, preserves the reasoning ability of R1-Zero while ensuring readability. All in all I\u0026rsquo;m very impressed by the DeepSeek team, and I hope you learnt something from this post!\nReferences References Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., \u0026amp; Guo, D. (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300 [cs.CL]. https://arxiv.org/abs/2402.03300\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., et al. (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL]. https://arxiv.org/abs/2501.12948\nDeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., et al. (2025). DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL]. https://arxiv.org/abs/2412.19437\nARENA curriculum. Transformer from Scratch. https://arena-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch\nInstead of adding KL penalty in the reward like in PPO, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of advantage $A$\u0026#160;\u0026#x21a9;\u0026#xfe0e;","href":"https://liuzihe02.github.io/posts/r1/","kind":"page","lang":"en","lastmod":"2025-02-17T17:05:08Z","objectID":"aea9ae4ea09924bdadd709902f9af108","publishDate":"2025-02-17T17:05:08Z","section":"posts","tags":[],"title":"Breaking Down DeepSeek R1","type":"posts"},{"content":"We explain how Monte Carlo Tree Search works and introduce its pseudocode. We borrow notes and figures from int8\u0026rsquo;s website and the MCTS Wikipedia page. I wrote a lightweight implementation for tic-tac-toe available here and a super lightweight implementation available here.\nGame Representation A game tree is a tree data structure, where every node represents a distinct state of the game. Given a state $s$, we take an action $a$, and the environment transitions to the new state $s'$ with the transition probability $p_a(s'|s)$. If the transition probablity is $1$ (such as in simple deterministic games like Tic Tac Toe), then each node in the game tree represents a state.\nWe use \u0026ldquo;action\u0026rdquo; and \u0026ldquo;move\u0026rdquo; interchangeably in the context of games\nThe root of the game tree represents the initial state of the game Any transition from one state to another consists a move The game ends at a terminal node Monte Carlo Tree Search Taken from CadiaPlayer Our game tree contains various connected game states, with each node in the tree containing statistics used to compute the value of a particular state. During \u0026ldquo;training\u0026rdquo;, MCTS traverses down the game tree from the root node $R$ until a leaf node $L$ is reached, then expands the game tree by adding a child node $C$.\nAfter expanding the game tree, we simulate the game from the child node until game termination at $T$ - a rollout. The results of the rollout is then used to update the game tree, so that we can choose better nodes to do rollouts from next time.\nThis way we make the game tree contain better estimates of the values of each states, which allows us to make better actions when we actually play the game. This occurs through the 4 stages: selection, expansion, simulation, and backpropagation.\nSelection The bold circles contain the nodes selected using the tree policy UCT. The color of the node represent whose turn it is to move next. The numbers in the nodes represent the statistics of the node. For example, a black circle with $7/10$ means black to move next, $7$ wins played from this state, total of $10$ visits at this state. Hence if there are no draws, white would have won $3$ times from this node.\nThe first phase, selection, works by starting at the root $R$ (initial state of game) and traversing down the game tree using the tree policy until a leaf node $L$. The tree policy used here is Upper Confidence Trees (UCT), which is UCB1 applied to trees:\n$$ a = \\arg\\max_{a \\in A(s)} \\left\\{ Q(s,a) + C_p\\sqrt{\\frac{\\ln N(s)}{N(s,a)}} \\right\\} $$ where $N(s)$ is the number of times a node has been visited, $N(s,a)$ is the number of times $a$ has been selected from this node, and $C_p$ is a exploration constant. Increasing $C_p$ will encourage more exploration, while decreasing it encourages a more greedy approach, but $\\sqrt2$ is often chosen.\nIf the transition probability is 1 - taking an action will deterministically move the state to another - then UCT simplifies to\n$$ UCT(v) = \\arg\\max_{i} \\left\\{ \\frac{w(v_i)}{N(v_i)} + C_p \\sqrt{\\frac{\\ln N(v) }{N(v_i)}} \\right\\} $$ for a parent node $v$, child node $v_i$, and number of wins $w$ from this child node $v_i$. UCT It is a sum of two components – the first component of our function $ \\frac{w(v_i)}{N(v_i)} $, also called exploitation component, can be read as a win rate. The term on the right is the exploration component - encouraging child nodes that have little visits. During selection, we want to allow some exploration, hence $C_p\u003e0$ here.\nA leaf node $L$ here is a node in the game tree, where not all actions have been explored. In other words, it still has unexplored game states (if one action transitions to a state deterministically). Once a leaf node has no more unexplored actions, it is fully expanded.\nExpansion $3/3$ still has unexplored actions, hence we add a child node to it. This action then becomes \u0026ldquo;explored\u0026rdquo;\nThe second phase expansion occurs when we\u0026rsquo;ve selected a leaf node $L$ to expand on. An unexplored node is randomly chosen, and is added to the game tree as a child node.\nSimulation Once the new child is added to the tree, we do one rollout from the $0/0$ until game termination\nWe do one simulation (rollout) from the new child node $C$ until game termination, where moves are chosen according to the rollout policy. This rollout policy is often completely random.\nBackpropagation We can see only black nodes have their win count updated.\nAfter the simulation reaches an end, all of the nodes taken in this path are updated. All of the nodes number of visits are incremented by one each, and the colour that matches the winner will have its win count incremented by 1. Hence useful statistics to keep are the number of visits and win count for that player.\nIn this repo, we store the win counts of both players in each node. A node also stores the board state, which contains which player it is to move next\nPseudoCode def select(node): while node is not terminal: if node is not fully expanded (has unexplored actions): return node else: node = UCT(node) return node def expand(node): assert unexplored_actions\u0026gt;0 action = unexplored_actions.pop() add this action as a child (to the game tree) return child def simulate(node): while state is not terminal choose an action (rollout policy) move the state return the result of game def backpropagate(node,result): update this node if parent exists: backpropagate(parent,result) def UCT(node,c): choices = (child.value/child.visits)+ c*np.sqrt((np.log(parent.visits)/child.visits)) return node.children[argmax(choices)] def train(root): leaf=select(root) if leaf has unexplored actions: child=expand(leaf) reward=simulate(child) backpropagate(child,reward) else: reward=simulate(leaf) backpropagate(leaf, reward)","href":"https://liuzihe02.github.io/posts/mcts/","kind":"page","lang":"en","lastmod":"2024-12-07T17:05:08Z","objectID":"4866ff99ed4a5a615c61a11335a5d2e8","publishDate":"2024-12-07T17:05:08Z","section":"posts","tags":[],"title":"Breaking Down Monte Carlo Tree Search","type":"posts"}]