[{"content":"I\u0026rsquo;ve wondered about entropy for a long time. From thermodynamics and the arrow of time, to cross-entropy loss in ML cost functions, or KL divergence in stopping intelligence from going batshit crazy in VAE/PPO, entropy keeps cropping up. Maybe the universe is playing some kinda trick on us here. Since I\u0026rsquo;m taking 3F7 Information Theory, I thought it\u0026rsquo;s probably a good time to understand what these ideas really mean.\nDisclaimer: I do not claim this work to be my own; this is merely a synthesis of intuition and math across multiple resources for myself and anyone interested. In particular, we borrow the beautiful figures and analogies from Chris Olah\u0026rsquo;s amazing post1 on Information Theory.\nContents 1 Entropy and Self-Information 1.1 Optimal Encoding 1.2 Source Coding Theorem 2 Cross-Entropy 3 KL Divergence 4 Mutual Information 5 Language Modelling 5.1 Cross-Entropy, Maximum Likelihood Estimation, Multi-class Classification 5.2 Auto-regressive Language Modelling 5.3 Information Theory of LLMs 6 Conclusion References 1 Entropy and Self-Information We introduce entropy as the fundamental limit to compressing information.\n1.1 Optimal Encoding Encoding\nSuppose my friend Jerry is at the London Zoo, while I\u0026rsquo;m (Zach) at the Singapore Zoo. There\u0026rsquo;s only 4 kinds of animals in both zoos: \u0026ldquo;dog\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;fish\u0026rdquo; and \u0026ldquo;bird\u0026rdquo;. Today, he\u0026rsquo;s telling me which animals he saw at the London Zoo - but for whatever reason he can only communicate in binary (0s and 1s).\nJerry likes to talk in binary To communicate, Jerry and I have a secret code mapping animals to sequences of bits. We use a simple mapping, which uses 2 bits to encode 4 animals/symbols:\nA mapping between animal symbols and binary codewords Technically, the animals are symbols $x_i$, with their collective called an alphabet $\\{x_1 \\dots x_i \\dots x_n\\}$. Their respective binary representations are called \u0026ldquo;codewords\u0026rdquo; and the original string message is \u0026ldquo;encoded\u0026rdquo; into its binary represenation.\nSome terminology Variable Encoding\nAll animals are equal, but some animals are more equal than others.\nThere are different numbers for each animal type in the London Zoo. We can plot a diagram showing the probability distribution of the animals and the length of our codeword representations:\nProbability distribution of animals in the London Zoo Vertical axis shows the probability of each animal $p(x_i)$, while the horizontal axis shows the length of the corresponding codeword $L(x_i)$ The vertical axis shows the probability $p(x_i)$ of each animal seen by Jerry, while the horizontal axis shows the length of the corresponding codeword $L(x_i)$ in our secret code mapping. Notice the total area summed and normalized over all codewords $\\sum_{i} p(x_i) L(x_i)$ is in fact the average length $\\mathbb{E}[L(X)]$ codewords in our mapping!\nInformation is expensive - we\u0026rsquo;d like to minimize this average length $\\mathbb{E}[L(X)]$. Let\u0026rsquo;s exploit this uneven-ness in the probability distribution, and assign shorter codewords to more frequent animals:\nOur new codeword mapping with shorter codewords for more frequent animals (like dogs) Visualizing $p(x_i)$ and $L(x_i)$ for our new codeword mapping The average length of codewords (total area above) went down from 2 bits to 1.75 bits! How much further can we reduce this? Turns out there\u0026rsquo;s a fundamental limit - we call this the entropy of the probability distribution. To understand why such a limit exists and how to compute it, we need to understand the tradeoffs between making some codewords short and others long.\nTradeoffs in the Space of Codewords, Kraft\u0026rsquo;s Inequality\nTo understand this tradeoff, we visualize all possible codewords first on a table and then on a tree:\nAll possible codewords as a table All possible codewords of length $L_{max}=3$, with the number of leaves on the tree as $2^{L_{max}}$ We need our code to be uniquely decodable, which means that given an encoded string there is only one way to decode it back to our animal symbols. (e.g. an alphabet $\\{0, 1, 01\\}$ as codewords is not uniquely decodable because 001 could mean $[0,0,1]$ or $[0,01]$) One way to achieve this property is to create a prefix-free code, which means no codeword can be the prefix of another codeword. In terms of trees, this means every codeword must be a leaf node. By enforcing this prefix-tree condition, we have to make tradeoffs whenever we use a codeword. Consider using $01$ as a codeword:\nSacrificing $010$ and $011$ Pruning a tree A quarter of codewords $\\{010, 011\\}$ (assuming we only used 3bit codewords) would be gone from the space of codewords. We see that a shorter codeword requires you to sacrifice more of the space of possible codewords. In fact we can quantify this tradeoff as Kraft\u0026rsquo;s inequality:\nConsider a full binary tree of depth $L_{max}$, which has $2^{L_{max}}$ leaves. Whenever we use a codeword $x_i$ at depth $L(x_i)$, we eliminate all its descendant leaves - that\u0026rsquo;s $2^{L_{max} - L(x_i)}$ leaves becoming unavailable. For a prefix-free code with codewords at depths $L_1, L_2, \\ldots, L_n$, the total number of unusable leaves is:\n$$\\sum_{i=1}^{n} 2^{L_{max} - L(x_i)}$$ This sum cannot exceed the total number of leaves $2^{L_{max}}$:\n$$\\sum_{i=1}^{n} 2^{L_{max} - L(x_i)} \\leq 2^{L_{max}}$$ Dividing both sides by $2^{L_{max}}$:\n$$\\sum_{i=1}^{n} 2^{-L(x_i)} \\leq 1$$ This is Kraft\u0026rsquo;s inequality. It\u0026rsquo;s both a necessary condition (any prefix-free code must satisfy it) and a sufficient condition (if a set of lengths satisfies it, we can construct a prefix-free code with those lengths). This inequality formalizes the tradeoff in the space of codewords - shorter codewords are \u0026ldquo;expensive\u0026rdquo; as they eliminate more possibilities. Assuming a total space of $1$, each codeword $x_i$ of length $L(x_i)$ costs $2^{-L(x_i)}$ of the total space of codewords. We can think of the total space as a maximum fixed \u0026ldquo;budget\u0026rdquo; of $1$, and each codeword costing $2^{-L(x_i)}$ of this budget.\nOptimal Encodings\nWe\u0026rsquo;ve showed the cost of codewords decreases exponentially with its length. We can visualize this 2 with the cost on the vertical axis and the length on the horizontal axis of an exponential curve:\nVisualizing the cost of a codeword against its length. Both the height and right shaded area are numerically equal. Note that since cost decays as a (natural) exponential, it is both the height of the right shaded area and the actual shaded area2. We can also visualize the contribution $p(x_i) L(x_i)$ of this codeword to the average codeword length:\n$$\\mathbb{E}[L(X)]=\\sum_{i} p(x_i) L(x_i)$$ Contribution $p(x_i) L(x_i)$ of codeword $x_i$ to the average codeword length $\\mathbb{E}[L(X)]$ Visualizing both the cost and length contribution we get:\nFor codeword $x_i$, contribution to average codeword length is the left area, while cost of codeword space is the right area Short codewords reduce the average message length but are expensive, while long codewords increase the average message length but are cheap. The above diagram makes the tradeoff clear: short codewords reduce the average message length but are expensive, while long codewords increase the average message length but are cheap. So what\u0026rsquo;s the optimal allocation of the codeword space budget to each symbol $x_i$, so that the average codeword length is minimized? We propose that if a symbol (or event) $x_i$ appears with probability $p(x_i)$, then the optimal allocation is to allocate $p(x_i)$ of the codeword space budget to this symbol. Here\u0026rsquo;s a handwavy proof:3\n$$ \\text{Cost}(x_i)=2^{-L(x_i)}=p(x_i), \\forall i $$ $$ \\frac{d\\big(\\text{Length Contribution}(x_i)\\big)}{dL(x_i)} = \\frac{d\\big(p(x_i) L(x_i)\\big)}{dL(x_i)} = p(x_i) $$ $$ \\frac{d\\big(\\text{Cost}(x_i)\\big)}{dL(x_i)} = \\frac{d\\big(2^{-L(x_i)}\\big)}{dL(x_i)} = -2^{-L(x_i)} \\ln(2) $$ At the optimal allocation where $2^{-L(x_i)} = p(x_i)$,\n$$\\frac{|\\text{Length Contribution}{(x_i)}|}{|\\text{Cost}{(x_i)}|} = \\frac{\\text{Benefit from lower length contribution}}{\\text{Cost increase from }{x_i}} = \\frac{1}{\\ln(2)}$$ which is constant regardless of $p(x_i)$. This means at the optimal budget, it\u0026rsquo;s equally worthwhile to invest in making any codeword shorter as all codewords have the same benefit/cost ratio. Note that the actual value of this hypothetical benefit/cost ratio doesn\u0026rsquo;t matter (the top and bottom quantities don\u0026rsquo;t have the same units so we cannot compare them meaningfully in terms of absolute values) - what matters is the relative ratios between the different symbols.\nIf $2^{-L(x_i)} \\ne p(x_i)$, then this would mean the ratio for each ${x_i}$ would in fact be dependent on both $L(x_i)$ and $p(x_i)$, and will result in uneven - hence suboptimal allocation of budgets. Any uneven-ness of this ratio means it will make more sense to spend more budget in one codeword than other codewords.\nCalculating Entropy\nSince the cost of an encoded message of length $L(x_i)$ is $2^{-L(x_i)}$, then $L(x_i)=\\log_2\\left(\\frac{1}{\\text{Cost}}\\right)$. Since we allocated $p(x_i)$ of the budget to $L(x_i)$ for the optimum allocation, then\n$$L(x_i)=\\log_2\\left(\\frac{1}{p(x_i)}\\right)$$ $$\\mathbb{E}[L(X)] = \\sum_{i} p(x_i) L(x_i) = \\sum_{i} p(x_i) \\log_2\\left(\\frac{1}{p(x_i)}\\right) $$ Kraft\u0026rsquo;s inequality created a budget constraint on how many short codewords you can have. The optimal strategy is to spend this budget proportionally to symbol probabilities. Any other allocation either violates the constraint (making the code invalid, like not prefix-free) or wastes budget (making the average length longer). Entropy emerges because it\u0026rsquo;s the only allocation that balances the budget constraint with minimizing average length!\nHence for a probability distribution of symbols, the minimum average length across all the encoded symbols is the entropy:\n$$ H(X) = \\min_{\\text{all encodings}} \\mathbb{E}[L(X)] = \\sum_{i} p(x_i) \\log_2\\left(\\frac{1}{p(x_i)}\\right) $$ The more concentrated the probability, the easier it is to exploit this by crafting short encodings. The more spread out the probability, the less I am able to exploit this, and I have to make all my encodings longer on average. So a distribution with higher uncertainty requires more bits to communicate information.\n1.2 Source Coding Theorem We\u0026rsquo;ve shown the intuition that the optimal codeword allocation follows $2^{-L(x_i)} = p(x_i)$, yielding entropy. Here\u0026rsquo;s the actual proof for this - Shannon\u0026rsquo;s source coding theorem. We\u0026rsquo;ve seen that prefix-free codes obeys the budget constraint $\\sum_{i} 2^{-L(x_i)} \\leq 1$, and McMillan4 proved the same constraint holds for all uniquely decodable codes (of which prefix-free codes are just a subset). Consider a general (possibly non-optimal) coding scheme giving length $L(x_i)$ for symbol $x_i$:\n$$H(X) - \\mathbb{E}[L(X)] = \\sum_{i} p(x_i) \\log_2\\left(\\frac{1}{p(x_i)}\\right) - \\sum_{i} p(x_i) L(x_i)$$ $$= \\sum_{i} p(x_i) \\log_2\\left(\\frac{2^{-L(x_i)}}{p(x_i)}\\right)$$ $$= \\frac{1}{\\ln 2} \\sum_{i} p(x_i) \\ln\\left(\\frac{2^{-L(x_i)}}{p(x_i)}\\right)$$ Note for any $x \u003e 0$, we have $\\ln(x) \\leq x - 1$ (with equality only when $x = 1$). This gives us:\n$$H(X) - \\mathbb{E}[L(X)] \\leq \\frac{1}{\\ln 2} \\sum_{i} p(x_i) \\left(\\frac{2^{-L(x_i)}}{p(x_i)} - 1\\right)$$ $$\\frac{1}{\\ln 2} \\left(\\sum_{i} 2^{-L(x_i)} - 1\\right) \\leq 0$$ where we\u0026rsquo;ve used McMillan\u0026rsquo;s constraint $\\sum_i 2^{-L(x_i)} \\leq 1$. Therefore no encoding can beat entropy $ H(X) \\leq \\mathbb{E}[L(X)] $.\nShannon\u0026rsquo;s Source Coding Theorem\nFor any uniquely decodable code encoding symbols (where $L(X)$ gives the length of the codeword for symbol $X$) from a distribution with entropy $H(X)$:\n$$H(X) \\leq \\mathbb{E}[L(X)]$$ This limit explains why:\nRandom data doesn\u0026rsquo;t compress — it has maximum entropy because all symbols are equally likely, we can\u0026rsquo;t exploit shorter messages for more common events English text compresses well — it has entropy $\\sim 1.5$ bits per letter (much less than uniform distribution $\\log_2(26) \\approx 4.7$) because letter probabilities are wildly uneven 2 Cross-Entropy Turns out the distribution of animals is different in the London Zoo $P$ and Singapore Zoo $Q$:\nAnimal distributions differ: London Zoo has more dogs, Singapore Zoo has more cats What if I design my code mapping to be locally optimal for the Singapore Zoo, but Jerry is actually sending me data from the London Zoo overseas? That is, I optimize codeword lengths $L(x_i) = \\log_2\\left(\\frac{1}{q(x_i)}\\right)$ for distribution $Q$ (Singapore), but the actual data comes from distribution $P$ (London). The expected codeword length becomes:\n$$H_P(Q) = \\sum_i p(x_i) \\log_2\\left(\\frac{1}{q(x_i)}\\right)$$ We call this the cross-entropy from $P$ to $Q$. It represents the average length of encoded messages for data actually drawn from distribution $P$, using an optimal code designed for distribution $Q$. Since we have used the unoptimized code mapping on distribution $P$, we have to pay a cost through an increase in the average length of encoded messages $H_P(Q) \\geq H(P)$:\n$$H_P(Q) = H(P) + D_{KL}(P \\| Q)$$ Cross Entropy measures the average length of encoded messages drawn from $P$ using the encoding of $Q$, which is longer than the encoding optimize for $P$ $H(P)$: The irreducible minimum—the entropy of the true underlying distribution $D_{KL}(P \\| Q)$: The penalty for mismatch—how much extra bits we waste by optimizing for the wrong distribution This mismatch means we waste bits on unexpected events (where $p(x_i) \u003e q(x_i)$) while potentially using longer-than-necessary codes for common events (where $p(x_i) \u003c q(x_i)$), hence giving a length penalty. When $Q = P$ (our assumption is perfect), the second term vanishes and cross-entropy equals entropy—we\u0026rsquo;re encoding optimally. When they differ, $D_{KL}(P \\| Q) \u003e 0$ quantifies exactly the penalty. In this sense, cross-entropy gives us a measure of how 2 different probability distributions are. The more different 2 distributions are, the bigger the cross-entropy $H_P(Q)$.\nAsymmetry\nCross-entropy is fundamentally asymmetric $H_P{(Q)} \\ne H_Q{(P)}$:\nCross-entropy computed in various ways $H_P{(Q)}$ is large because we pay a very large penalty using a long codeword (that\u0026rsquo;s rare in $Q$ but common in $P$) for the purple symbol. On the other hand, this extra cost for using unnecessarily longer codewords when encoding real data from $Q$ is not quite as big.\n3 KL Divergence Now that we understand cross-entropy, we can isolate the pure cost of mismatch. The Kullback-Leibler (KL) divergence is exactly this: cross-entropy minus the intrinsic entropy.\n$$D_{KL}(P \\| Q) = H_P(Q) - H(P) = \\sum_i p(x_i) \\log\\left(\\frac{p(x_i)}{q(x_i)}\\right)$$ This is the extra bits we pay because we optimized for the wrong distribution $Q$ instead of the the distribution where samples came from $P$. Note the the following properties:\nNon-negativity: $D_{KL}(P \\| Q) \\geq 0 \\text{, with equality iff } P = Q$ Not really a metric 5 6 Asymmetry $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$ 7 Forward KL: $D_{KL}(P \\| Q)$ (what we usually minimize) is mean-seeking. If $P$ is bimodal, and we optimize $Q$ to reduce KL, then $Q$ diffuses to cover both modes Reverse KL: $D_{KL}(Q \\| P)$ is mode-seeking. To minimize KL, $Q$ concentrates probability around the 2 modes 4 Mutual Information Mutual information measures how much one variable reduces the uncertainty about another:\n$$I(X; Y) = D_{KL}(P(X,Y) \\| P(X)P(Y))$$ This compares the joint distribution $P(X,Y)$ against what we\u0026rsquo;d expect if $X$ and $Y$ were independent: $P(X)P(Y)$. When variables are independent, the joint factorizes and the KL divergence is zero—they share no information. When they\u0026rsquo;re dependent, $I(X; Y) \u003e 0$ quantifies the dependence. Using the decomposition $H_P(Q) = H(P) + D_{KL}(P \\| Q)$, we can also expand:\n$$I(X; Y) = H(X) - H(X|Y)$$ This form says: \u0026ldquo;mutual information is the reduction in uncertainty about $X$ when we learn $Y$.\u0026rdquo; Since $H(X|Y)$ is the conditional entropy (uncertainty about $X$ after knowing $Y$), the difference $H(X) - H(X|Y)$ measures exactly how much information $Y$ gave us about $X$.\nBy the chain rule of entropy $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$ we can derive:\n$$I(X; Y) = H(Y) - H(Y|X)$$ Combining both:\n$$I(X; Y) = H(X) + H(Y) - H(X,Y)$$ Note the following properties:\nSymmetry: $I(X; Y) = I(Y; X)$ knowing $Y$ tells you as much about $X$ as knowing $X$ tells you about $Y$ Non-negativity: $I(X; Y) \\geq 0 $ Due to non-negativity of $D_{KL}$ With equality iff $X$ and $Y$ are independent Self-Information: $I(X; X) = H(X)$ This is why entropy is sometimes called \u0026ldquo;self-information\u0026rdquo;—it\u0026rsquo;s just mutual information of a variable with itself. Note that $P(X,X)=P(X)$. Here\u0026rsquo;s the full picture:\nDiagram of entropy relationships. H(X) and H(Y) are marginal entropies, H(X,Y) is joint entropy, I(X;Y) is mutual information shown as overlap When $X$ and $Y$ are independent, there\u0026rsquo;s no overlap—$I(X;Y) = 0$. When they\u0026rsquo;re perfectly correlated, they share all their information—$I(X;Y) = H(X) = H(Y)$.\n5 Language Modelling 5.1 Cross-Entropy, Maximum Likelihood Estimation, Multi-class Classification The cross entropy is often used as an objective/loss function in multi-class classification, when training machine learning (ML) or large language models (LLM). This objective is essentially minimizing the distance between our model\u0026rsquo;s distribution and the ground truth distribution:\n$P$ = true labels (the ground truth data distribution) $Q$ = predicted probabilities (our model\u0026rsquo;s beliefs) Minimizing cross-entropy is equivalent to maximum likelihood estimation (MLE)! Given a sample dataset $\\{x_1, \\ldots, x_n\\}$ (sample datapoints not the alphabet here) drawn from true distribution $P$, define the likelihood of data under our model with parameters $\\theta$:\n$$\\mathcal{L}(\\theta) = \\prod_{i=1}^{n} q_\\theta(x_i)$$ We seek\n$$\\hat{\\theta} = \\arg\\max_\\theta \\mathcal{L}(\\theta) = \\arg\\max_\\theta \\prod_{i=1}^{n} q_\\theta(x_i) $$ Convert the product to a sum via logarithms (since logarithms are strictly increasing and do not affect argmax), then normalize by sample count:\n$$\\hat{\\theta} = \\arg\\max_\\theta \\frac{1}{n}\\sum_{i=1}^{n} \\log q_\\theta(x_i)$$ By the Law of Large Numbers, this sample average converges to an expected value as $n \\to \\infty$. Specifically, the expectation is with respect to the true underlying distribution $P$:\n$$\\frac{1}{n}\\sum_{i=1}^{n} \\log q_\\theta(x_i) \\to \\mathbb{E}_{x \\sim P}\\left[\\log q_\\theta(x)\\right] = \\sum_{i} p(x_i) \\log q_\\theta(x_i)$$ Negate and minimize:\n$$\\hat{\\theta} = \\arg\\min_\\theta \\left\\{ -\\sum_{i} p(x_i) \\log q_\\theta(x_i) \\right\\} = \\arg\\min_\\theta H_P(Q_\\theta)$$ MLE is equivalent to minimizing cross-entropy!\nNote that minimizing cross-entropy is also equivalent to minimizing the KL divergence $D_{KL}(P \\| Q)$. Since $H(P)$ is the entropy of the true distribution, it\u0026rsquo;s independent of the model parameters $\\theta$.\n$$\\arg\\min_\\theta H_P(Q_\\theta) = \\arg\\min_\\theta [D_{KL}(P \\| Q_\\theta) + H(P)] = \\arg\\min_\\theta D_{KL}(P \\| Q_\\theta)$$ MLE in Practice\nIn practice, we don\u0026rsquo;t have access to the true distribution $P$. Instead, we can approximate this by using a batch of sample data. Consider a classification batch of $n$ samples, where each sample $i$ has a true label $y_i$ (one-hot: probability 1 for the true class, 0 elsewhere) and the model predicts $q_\\theta(y=k)$ for each class $k$. The batch loss is:\n$${L}(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\log q_\\theta(y_i)$$ Note in practice we only sum the model log probabilities for the class corresponding to the true class for each sample, despite having log probs for all classes. Now regroup by class with $n_k$ samples in each class $k$, so we can rewrite the sum as:\n$$ {L}(\\theta) = -\\frac{1}{n} \\sum_{k=1}^{K} n_k \\log q_\\theta(y=k) = -\\sum_{k=1}^{K} \\frac{n_k}{n} \\log q_\\theta(y=k)$$ Note that $\\hat{p}(y=k) = \\frac{n_k}{n}$ is the empirical frequency of class $k$ in the batch\n$$ {L}(\\theta) = -\\sum_{k=1}^{K} \\hat{p}(y=k) \\log q_\\theta(y=k) = H_{\\hat{P}}(Q_\\theta)$$ This is exactly the cross-entropy formula! One-hot labels, when averaged over a batch, naturally produce an empirical distribution. With sufficient data, $\\hat{p}(y=k) \\to p(y=k)$ (the true class distribution), so batch averaging realizes the theoretical MLE objective.\n5.2 Auto-regressive Language Modelling So we\u0026rsquo;ve seen how cross-entropy applies to multi-class classification. We now extend this to LLMs that generate long sequences of text. Consider a sequence of tokens $X=(x_1, x_2, \\dots, x_T)$. The objective of LLMs is to model the joint probability distribution of this sequence $p(x_1, x_2, \\dots, x_T)$. We can use the chain rule of probability to factor this joint distribution into a product of conditional probabilities:\n$$p(x_1,\\ldots,x_T) = p(x_1) \\cdot p(x_2 | x_1) \\cdot p(x_3 | x_1, x_2) \\cdots = \\prod_{t=1}^T p(x_t|x_1,\\ldots,x_{t-1})$$ An autoregressive LLM is just a sequence of simple classification tasks! At each timestep $t$,\nIt takes the context $x_{\\lt t}$ (e.g., \u0026ldquo;The quick brown fox\u0026hellip;\u0026rdquo;). It predicts a probability distribution $Q_\\theta(\\cdot | x_{\\lt t})$ over the entire vocabulary (e.g., 50,000 possible next tokens). Each token is a class. The \u0026ldquo;true label\u0026rdquo; $P_t$ is simply the one-hot vector for the token that actually comes next in the training data (e.g., \u0026ldquo;jumps\u0026rdquo;). The total loss for one sequence ${L}_X(\\theta)$ is just the sum of the cross-entropy losses across all timesteps:\n$${L}_X(\\theta) = \\sum_{t=1}^T -\\log q_\\theta(x_t | x_{\\lt t}) $$ This sum of losses is equivalent to the negative log-likelihood of the entire sequence $X$:\n$${L}_X(\\theta) = -\\sum_{t=1}^T \\log q_\\theta(x_t | x_{\\lt t}) = -\\log \\left( \\prod_{t=1}^T q_\\theta(x_t | x_{\\lt t}) \\right) = -\\log q_\\theta(X)$$ LLMs are just doing MLE on the joint distribution of the entire sequence.\n5.3 Information Theory of LLMs Prediction is essentially compression.\nGeneralization\nInformation theory can help us analyze generalization in LLMs, by measuring a model\u0026rsquo;s capacity in bits [Morris, 2025]. Morris splits learned information into \u0026ldquo;unintended memorization\u0026rdquo; (information specific to dataset) and \u0026ldquo;generalization\u0026rdquo; (underlying distribution). They then compute the total capacity of LLMs, where models in the GPT family have an approximate capacity of 3.6 bits-per-parameter. When a model is trained on data whose information content exceeds its capacity, it must make a tradeoff between memorizing individual datapoints, or begin extracting general patterns that compress the data more efficiently. This transition point—where unintended memorization gives way to generalization—occurs precisely when the dataset\u0026rsquo;s entropy exceeds the model\u0026rsquo;s capacity. This explains phenomena like \u0026ldquo;grokking\u0026rdquo; and double descent.\nInformation in Chain-Of-Thought\nWhen an LLM uses Chain-of-Thought (CoT) to solve a problem, it generates intermediate steps. But are these steps actually useful? Researchers quantify [Ton, 2025] the usefulness of each reasoning step ($X_t$) by measuring its \u0026ldquo;information gain\u0026rdquo;—the conditional mutual information it shares with the final correct answer ($Y$), given all the previous steps ($X_{\\lt t}$):\n$$G_t = I(Y; X_t | X_{ \\lt t})$$ This metric basically answers: \u0026ldquo;How much new and relevant information did this step provide about the final answer, given everything the model has already said?\u0026rdquo;\n6 Conclusion Whether we\u0026rsquo;re designing optimal codes for data compression, training models to predict the next token, or evaluating multi-step reasoning, we return to the same fundamental question: How much information is really here? As we\u0026rsquo;ve seen throughout this post, entropy—in all its forms—is often the answer.\nReferences Entropy, Cross-Entropy, KL Divergence\nZhu, J. Information Theory and Machine Learning. University of Wisconsin-Madison, CS 769 Lecture Notes.\nSarang, N. (2024). Entropy, Cross-Entropy, KL Divergence, and Mutual Information.\nJames Explains. (2024). Information Theory - Entropy, Cross Entropy, KL Divergence [Video]. YouTube.\nBendersky, E. (2025). Cross-Entropy and KL Divergence. Eli Bendersky\u0026rsquo;s Website.\nMaulik, A. Entropy. Indian Institute of Technology Guwahati, CS Lecture Slides.\nOlah, C. (2015). Visual Information Theory.\nGoodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep Learning (Chapter 3: Probability and Information Theory). MIT Press.\nTan, Z., Li, C., \u0026amp; Huang, W. (2024). The Information of Large Language Model Geometry.\nTon, J.F., Taufiq, M.F., \u0026amp; Liu, Y. (2025). Understanding Chain-of-Thought in LLMs through Information Theory. ICML 2025\nMorris, J. X., Sitawarin, C., Guo, C., Kokhlikyan, N., Suh, G. E., Rush, A. M., Chaudhuri, K., \u0026amp; Mahloujifar, S. (2025). How much do language models memorize?.\nSource Coding Theorem\nWikipedia. Kraft–McMillan Inequality.\nWikipedia Shannon's Source Coding Theorem.\nBernstein, M. Shannon's Source Coding Theorem. Matthew Bernstein\u0026rsquo;s Blog.\nSingh, A. (2015). Information Theory. Carnegie Mellon University, 10-704 Lecture Notes.\nVenkataramanan, R. (2025). 3F7: Information Theory and Coding - Handouts 2 \u0026 3. University of Cambridge, Department of Engineering.\nOlah, C. (2015). Visual Information Theory. Chris Olah does an awesome job explaining the intuition behind entropy and its related concepts\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTechnically the equal area math only holds if we work with natural logs instead of base 2 logs as $\\int_{x'}^{\\infty} 2^{-x} \\, dx = \\frac{2^{-x'}}{\\ln 2}$ We are working with base 2 logs instead of natural logs, but we omit the constant scaling factor in our calculations if not they will look rather messy.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI chose not to use Olah\u0026rsquo;s visual proof exactly here because his proof suggests that we need the benefit/cost ratio to be 1. He suggests any deviation in the allocation of budget will change the ratio to not be 1, which is not exactly the right argument. The important thing is that all symbols have the same benefit/cost ratio, so the allocation of budget is optimum.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee Kraft–McMillan Inequality (1956).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA true metric must satisfy symmetry: $d(A, B) = d(B, A)$ and the triangle inequality: $d(A, C) \\leq d(A, B) + d(B, C)$. KL divergence doesn\u0026rsquo;t satisfy either property, so it is not really a proper metric for measuring distances between distributions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSome researchers define the Jensen-Shannon divergence as a symmetric alternative: $\\text{JS}(P, Q) = \\frac{1}{2}D_{KL}(P \\| M) + \\frac{1}{2}D_{KL}(Q \\| M)$ where $M = \\frac{P + Q}{2}$. This is a true metric (and $\\sqrt{\\text{JS}}$ is a metric), but it obscures the directed nature of the problem. For machine learning, forward KL is almost always the right choice.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee Nima Sarang's post on Information Theory.\u0026#160;\u0026#x21a9;\u0026#xfe0e;","href":"https://liuzihe02.github.io/posts/info/","kind":"page","lang":"en","lastmod":"2025-10-25T16:34:06.888Z","objectID":"2c51252e94219472b01971aa710e7c10","publishDate":"2025-10-25T16:34:06.888Z","section":"posts","tags":[],"title":"On Information","type":"posts"},{"content":"We analyze 1 the theoretical properties of both the Discrete Fourier Transform (DFT) and the optimized Fast Fourier Transform (FFT), and estimate their algorithmic complexity.\nAll code available on Github. Slides available here.\nDiscrete Fourier Transform Theory The DFT converts a finite-length time-domain signal into its frequency-domain representation. For an input signal $x[n]$ of length $N$, where $0 \\le n \\le N-1$, the DFT is defined as:\n$$ X[k] = \\sum_{n=0}^{N-1} x[n]\\, W_N^{nk}, \\quad 0 \\le k \\le N-1 \\label{eq:DFT} $$ where $W_N=e^{-j\\frac{2\\pi}{N}}$ is the $N$-th principal root of unity, and $X[k]$ is component at frequency $kf_s/N$ with sampling frequency $f_s$. We can rewrite $X[k]$ as an inner product:\n$$ \\begin{align} X[k] \u0026= \\left[ 1 \\quad e^{-j\\frac{2\\pi k}{N}} \\quad \\ldots \\quad e^{-j\\frac{2\\pi k}{N}(N-1)} \\right] \\begin{bmatrix} x[0] \\\\ x[1] \\\\ \\vdots \\\\ x[N-1] \\end{bmatrix} \\\\ \u0026= \\left[ 1 \\quad W_N^k \\quad \\ldots \\quad W_N^{(N-1)k} \\right] \\begin{bmatrix} x[0] \\\\ x[1] \\\\ \\vdots \\\\ x[N-1] \\end{bmatrix} \\end{align} $$ By varying $k$ from $0$ to $N-1$, we can collate all outputs ${X[k]}$ into a vector $\\mathbf{X}$, collate all inputs $x[n]$ into a vector $\\mathbf{x}$, to get:\n$$ \\mathbf{X}=\\mathbf{W}\\mathbf{x}, \\quad \\mathbf{W} = \\begin{bmatrix} 1 \u0026 1 \u0026 1 \u0026 \\cdots \u0026 1 \\\\ 1 \u0026 W_N \u0026 W_N^2 \u0026 \\cdots \u0026 W_N^{N-1} \\\\ 1 \u0026 W_N^2 \u0026 W_N^4 \u0026 \\cdots \u0026 W_N^{2(N-1)} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 1 \u0026 W_N^{N-1} \u0026 W_N^{2(N-1)} \u0026 \\cdots \u0026 W_N^{(N-1)(N-1)} \\end{bmatrix} $$ where $\\mathbf{W}$ is the DFT matrix.\nComplexity Pseudocode for DFT Direct implementation in MATLAB:\nfunction X = dft_loop(x) % Compute the Discrete Fourier Transform (DFT) of input vector x using naive loops % x : input signal (vector) % X : DFT of x % Convert input to a column vector for consistency x = x(:); N = length(x); % Number of samples in the signal % Pre-allocate the output vector for efficiency X = zeros(N, 1); % Loop over each frequency bin k (from 0 to N-1) for k = 0:N-1 sum_val = 0; % Loop over each time sample n (from 0 to N-1) for n = 0:N-1 % Compute and accumulate the contribution for the k-th frequency component sum_val = sum_val + x(n+1) * exp(-1j * 2 * pi * k * n / N); end % MATLAB uses one-based indexing, so assign to X(k+1) X(k+1) = sum_val; end end MATLAB code (vectorized):\nfunction X = dft_vectorized(x) % Compute the Discrete Fourier Transform (DFT) of input vector x using vectorized operations % x : input signal (vector) % X : DFT of x % Convert input to a column vector for consistency x = x(:); N = length(x); % Number of samples in the signal % Create index vectors: n as a row vector and k as a column vector n = 0:N-1; % Time indices (row vector) k = n\u0026#39;; % Frequency indices (column vector) % Construct the DFT matrix using the formula: exp(-1j*2*pi*k*n/N) W = exp(-1j * 2 * pi * k * n / N); % Multiply the DFT matrix with the signal to obtain the transform X = W * x; end Direct computation of $\\mathbf{X}$ requires $(N-1)^2$ complex multiplications and $N(N-1)$ complex additions. Since multiplication is more compute-intensive than addition, the asymptotic complexity is dominated by multiplication, giving $O(N^2)$.\nFast Fourier Transform Preliminaries We layout some properties of $W_N$ we\u0026rsquo;ll be using later\nProperty 1:\n$$ W_N^2 = W_{N/2} $$ Proof:\n$$ W_N^2 = e^{-j\\frac{2\\pi}{N}\\cdot 2} = e^{-j\\frac{2\\pi}{N/2}} = W_{N/2} $$ More generally, $W_N^{2nk} = W_{N/2}^{nk}$.\nProperty 2:\n$$ W_N^{k+\\frac{N}{2}} = -W_N^k $$ Proof:\n$$ W_N^{k+\\frac{N}{2}} = e^{-j\\frac{2\\pi}{N}\\left(k+\\frac{N}{2}\\right)} = e^{-j\\frac{2\\pi}{N}k} \\cdot e^{-j\\pi} = -W_N^k $$ Theory The radix-2 FFT algorithms work by dividing the DFT into 2 DFTs of length $N/2$ each, and iterating. We introduce the simplest variant, called the Decimation-In-Time (DIT) algorithm.\nConsider a $N$-point signal $x[n]$ of even length, indexed from $0$ to $N-1$. The derivation of the DIT radix-2 FFT begins by splitting the $x[n]$ into two parts \u0026mdash; one part for the even-indexed values $x[2n]$ and one part for the odd-indexed values $x[2n + 1]$. Define two $N/2$-point signals $x_{even}[n]$ and $x_{odd}[n]$ as $$x_{even}[n] = x[2n], \\quad x_{odd}[n] = x[2n + 1], \\quad 0 \\leq n \\leq N/2-1$$ The DFT can be written as\n$$ \\begin{aligned} X[k] \u0026= \\sum_{n=0 \\atop n \\text{ even}}^{N-1} x[n] W_N^{nk} + \\sum_{n=0 \\atop n \\text{ odd}}^{N-1} x[n] W_N^{nk}\\\\ \u0026= \\sum_{n=0}^{N/2-1} x[2n] W_N^{2nk} + \\sum_{n=0}^{N/2-1} x[2n + 1] W_N^{(2n+1)k}\\\\ \u0026= \\sum_{n=0}^{N/2-1} x_{even}[n] W_N^{2nk} + \\sum_{n=0}^{N/2-1} x_{odd}[n] W_N^{(2n+1)k} \\\\ \u0026= \\sum_{n=0}^{N/2-1} x_{even}[n] W_N^{2nk} + W_N^{k} \\cdot \\sum_{n=0}^{N/2-1} x_{odd}[n] W_N^{2nk}\\\\ \\end{aligned} $$ Noting that $W_N^{2}=W_{N/2}$ or more generally $W_N^{2nk}=W_{N/2}^{nk}$ , then\n$$ \\begin{aligned} X[k]\u0026= \\sum_{n=0}^{N/2-1} x_{even}[n] W_{N/2}^{nk} + W_N^{k} \\cdot \\sum_{n=0}^{N/2-1} x_{odd}[n] W_{N/2}^{nk} \\end{aligned} $$ Recognizing that the $\\frac{N}{2}$-point DFT of $x_{even}[n]$ and $x_{odd}[n]$ are given by:\n$$X_{even}[k] = \\text{DFT}_{\\frac{N}{2}}\\{x_{even}[n]\\} = \\sum_{n=0}^{N/2-1} x_{even}[n] W_{N/2}^{nk}$$ $$X_{odd}[k] = \\text{DFT}_{\\frac{N}{2}}\\{x_{odd}[n]\\} = \\sum_{n=0}^{N/2-1} x_{odd}[n] W_{N/2}^{nk}$$ we then obtain our core recursive equation:\n$$ X[k] = X_{even}[k] + W_N^{k} \\cdot X_{odd}[k] \\label{eq:fft_int} $$ Since $x_{even}[n]$ and $x_{odd}[n]$ are $N/2$-point signals with $0 \\leq n \\leq N/2-1$, their DFT are also only $N/2$-point signals. However, we require $0 \\le k \\le N-1$. We resolve this by noting their DFT coefficients are periodic with a period of $\\frac{N}{2}$:\n$$X_{even}[k] = X_{even}\\left[k + \\frac{N}{2}\\right], \\quad X_{odd}[k] = X_{odd}\\left[k + \\frac{N}{2}\\right]$$ This gives us\n$$ X[k] = \\begin{cases} X_{even}[k] + W_N^{k} \\cdot X_{odd}[k], \u0026 \\text{for } k = 0,1,\\ldots,\\frac{N}{2}-1 \\\\ X_{even}\\left[k-\\frac{N}{2}\\right] + W_N^{k} \\cdot X_{odd}\\left[k-\\frac{N}{2}\\right], \u0026 \\text{for } k = \\frac{N}{2},\\frac{N}{2}+1,\\ldots,N-1 \\end{cases} $$ Noting $W_N^{k +\\frac{N}{2}}=-W_{N}^{k}$, we write\n$$ X[k] = \\begin{cases} X_{even}[k] + W_N^{k} \\cdot X_{odd}[k], \u0026 \\text{for } k = 0,1,\\ldots,\\frac{N}{2}-1 \\\\ X_{even}\\left[k-\\frac{N}{2}\\right] - W_N^{k-\\frac{N}{2}} \\cdot X_{odd}\\left[k-\\frac{N}{2}\\right], \u0026 \\text{for } k = \\frac{N}{2},\\frac{N}{2}+1,\\ldots,N-1 \\end{cases} $$ finally giving us\n$$ \\begin{aligned} X[k] \u0026= X_{even}[k] + W_N^{k} \\cdot X_{odd}[k] \\quad \\text{for } 0 \\leq k \\leq \\frac{N}{2} - 1 \\\\ X[k + N/2] \u0026= X_{even}[k] - W_N^{k} \\cdot X_{odd}[k] \\quad \\text{for } 0 \\leq k \\leq \\frac{N}{2} - 1 \\label{eq:fft_final} \\end{aligned} $$ The multipliers $W_N^k$ are known as twiddle factors. The first computation with $+W_N^k$ give us the first half of the full DFT vector $\\mathbf{X}$, while the second computation with $-W_N^k$ give us the second half of $\\mathbf{X}$.\nComplexity Pseudocode for FFT MATLAB code for FFT:\nfunction X = fft_vectorized(x) % computes the radix-2 FFT recursively using vectorized operations. % If the length of x is not a power of 2, it is zero-padded to the next power of 2, added at the end x = x(:); % Ensure x is a column vector N = length(x); % If N is not a power of 2, zero-pad x to the next power of 2 M = 2^nextpow2(N); % nextpow2 returns the exponent so that 2^exponent \u0026gt;= N if M ~= N %add zeros to the end x = [x; zeros(M - N, 1)]; N = M; % Update N to the new length end % Base case: if the input length is 1, return x if N == 1 X = x; return; end % Recursively compute FFT for even and odd indices X_even = fft_vectorized(x(1:2:end)); %select all the even indices X_odd = fft_vectorized(x(2:2:end)); % selects all the odd indices % Compute twiddle factors (complex exponentials) in a vectorized manner % row array factor = exp(-1j * 2 * pi * (0:N/2-1).\u0026#39; / N); % Combine the FFTs of the even and odd parts using the butterfly operation X = [X_even + factor .* X_odd; X_even - factor .* X_odd]; end We can follow the above procedure to split an $N$-point DFT into two $\\frac{N}{2}$-point DFT, giving us the above algorithim. Let $A_c(N)$ and $M_c(N)$ denote respectively the number of complex additions and multiplications for computing the DFT of an $N$-point complex sequence $x[n]$. Let $N$ be a power of 2, $N = 2^k$. Then, we have $$A_c(N) = 2 A_c(N/2) + N \\quad , \\quad M_c(N) = 2 M_c(N/2) + \\frac{N}{2} - 1$$ as $N$ complex additions (addition of even and odd terms) and $\\frac{N}{2} - 1$ complex multiplications ($W_N^{k} \\cdot X_{odd}[k]$) are required to put the two $N/2$-point DFTs together. Note that a 2-point DFT is simply a sum and difference\n$$X[0] = x[0] + x[1] , \\quad X[1] = x[0] - x[1]$$ Hence, the starting conditions are $A_c(2) = 2$ and $M_c(2) = 0$. Solving the recursive equation yields $$A_c(N) = N \\log_2 N \\quad , \\quad M_c(N) = \\frac{N}{2} \\log_2 N - N + 1$$ Our overall complexity is $O(N\\log N)$.\nExperiments Complexity Execution time versus signal length $N$ for various implementations of DFT and FFT Algorithm O(N) O(N²) O(N³) O(log N) O(N log N) my-DFT 0.9208 0.9999 0.9864 0.4167 0.9417 my-FFT 0.9830 0.9657 0.9161 0.5634 0.9894 MATLAB-FFT 0.8491 0.7141 0.6526 0.8537 0.8287 To verify the theoretical complexity, we measured the execution time for computing the DFT over a range of signal lengths. For each signal length $N$, $N$ samples were taken from a 5Hz sine wave, and the DFT/FFT computation time recorded. We compare our implementation of DFT my-DFT, our implementation of FFT as my-FFT, and the MATLAB FFT implementation as MATLAB-FFT. You can view the measured execution times above on a log-log plot. The FFT results clearly exhibit an $O(N\\log N)$ scaling, while the DFT scales as $O(N^2)$. These experimental results confirm the significant computational advantage of using the FFT for large-scale problems. We also note the much more efficient implementation of MATLAB FFT, which uses the FFTW 2 package. The table above also shows the how well different models fit to data. Both FFT implementations fit well to $O(N)$ and $O(N\\log N)$ models.\nNumerical Error Reconstruction error versus signal length $N$ for various implementations of DFT and FFT We evaluate numerical accuracy of above algorithims by measuring reconstruction error of the reconstructed signal $\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}|x_i - \\hat{x}_i|^2}$ where $x_i$ is the original signal and $\\hat{x}_i$ is the reconstructed signal (using the MATLAB Inverse DFT function) Despite all errors falling within the $10^{-12}$ range, indicating high overall accuracy, the custom DFT implementation exhibits exponentially growing error with increasing sequence length. In contrast, both FFT implementations maintain consistently minimal error across all tested sequence lengths. The superior numerical stability of FFT algorithms is possibly due to its lower operation count (lower algorithmic complexity). Floating-point rounding errors accumulate more significantly in DFT.\nConclusion We study the theoretical motivations behind FFT. We verify that the computational complexity of the direct DFT implementation scales as $O(N^2)$, whereas our FFT implementation achieved the expected $O(N \\log N)$ complexity, offering a significant improvement in efficiency.\nReferences Golub, G. H., \u0026amp; Van Loan, C. F. (1996). Matrix computations (3rd ed.). Johns Hopkins University Press.\nStrang, G. (2007). Computational Science and Engineering. Wellesley-Cambridge Press. https://epubs.siam.org/doi/abs/10.1137/1.9780961408817\nRamalingam, C.S. Introduction to the Fast-Fourier Transform (FFT) Algorithm. https://www.ee.iitm.ac.in/~csr/teaching/pg_dsp/lecnotes/fft.pdf\nFast Fourier Transform (FFT). NYU Engineering. https://eeweb.engineering.nyu.edu/iselesni/EL713/zoom/fft\nThis post was created for educational purposes, so much of the analysis is taken directly from the sources quoted in the references\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis package is highly optimized to each machine and is written in low level C\u0026#160;\u0026#x21a9;\u0026#xfe0e;","href":"https://liuzihe02.github.io/posts/dft-fft/","kind":"page","lang":"en","lastmod":"2025-03-28T17:05:08Z","objectID":"edf0899851b8b46beb0b136c155139c4","publishDate":"2025-03-28T17:05:08Z","section":"posts","tags":[],"title":"The Discrete Fourier Transform and Fast Fourier Transform","type":"posts"},{"content":"In this post, I want to explore the key RL architecture and software optimizations DeepSeek made to create Deepseek-R1. I\u0026rsquo;ll provide a brief introduction to the RL setup and explain how GRPO makes traditional algorithims much more efficient. At the end, I\u0026rsquo;ll provide a brief overview of how DeepSeek-R1 was trained. Talk given at Cambridge AI Safety Hub Members Meeting in February, slides available here.\nPreliminaries Transformers Illustration of autoregressive model of transformers, taken from the ARENA curriculum Modern transformers are autoregressive models. They predict the next token (essetially a word) given all the tokens before it:\n$$p(x_1,\\ldots,x_N) = \\prod_{n=1}^N p(x_n|x_1,\\ldots,x_{n-1})$$ Transformer architecture overview, taken from the ARENA curriculum At a high level, input natural language are broken up into tokens, and converted to a numerical form. The output is then a probability distribution over the next possible token (from logits), and the next token is sampled from this distribution.\nReinforcement Learning We provide a brief overview of the RL setup.\nAn agent in state $s$ issues an action $a$ to the environment, and the environment replies with state, reward pairs $(s',r)$. We can then define a trajectory $\\tau$ as a sequence of states and actions: $s_0, a_0, r_1, a_1, r_2, s_2, a_2, r_3...$\nThe agent chooses actions using a policy $\\pi$ which can either be a deterministic function $a=\\pi(s)$ from states to actions, or more generally actions are sampled $a \\sim \\pi(\\cdot|s)$\nThe return at time $t$ is the sum of rewards obtained after time $t$ until the time $T$ when the terminal state is reached, discounted by how far into the future:\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} ...$$ The state value function $V_\\pi(s)$ is the expected return, if you start in state $s$ and always act according to policy $\\pi$:\n$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[G_t| s_t = s\\right]$$ We deal with these core quantities:\nThe action value function is the expected return if you start in state $s$, take an arbitrary action $a$ (which may not have come from the policy), and then afterwards forever act according to policy $\\pi$: $$Q_\\pi(s, a) = \\mathbb{E}_{\\pi} \\left[G_t| s_t = s, a_t = a\\right]$$ The advantage function describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to $\\pi(\\cdot|s)$, assuming you act according to $\\pi$ forever after:\n$$A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s)$$ Trust Region Policy Optimization (TRPO)\nLet $\\pi_\\theta$ denote a policy with parameters $\\theta$. We aim to maximize the expected return $J$ of policy $\\pi_\\theta$, where we take expectation over all possible trajectories $\\tau$ generated by following policy $\\pi_\\theta$\n$$J(\\pi_\\theta) = \\underset{\\tau \\sim \\pi_\\theta}{\\mathbb{E}}[G(\\tau)]$$ To achieve the optimum $\\pi_\\theta$ that maximizes J, we can iteratively update $\\pi_{\\theta_{k}}$ according to:\n$$\\theta_{k+1} = \\arg \\max_\\theta \\mathcal{L}(\\theta_k, \\theta) = \\arg \\max_\\theta \\underset{s,a\\sim\\pi_{\\theta_k}}{\\mathbb{E}} \\left[L(s, a, \\theta_k, \\theta)\\right], \\quad \\text{s.t. } \\bar{D}_{KL}(\\theta\\|\\theta_k) \\leq \\delta$$ where $\\mathcal{L}(\\theta_k, \\theta)$ is the surrogate advantage, a measure of how policy $\\pi_\\theta$ performs relative to the old policy $\\pi_{\\theta_k}$ using data from the old policy:\n$$L(s, a, \\theta_k, \\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)} A_{\\pi_{\\theta_k}}(s,a)$$ TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be.\nProximal Policy Optimization (PPO)\nProximal Policy Optimization updates policies via:\n$$\\theta_{k+1} = \\arg \\max_\\theta \\mathcal{L}(\\theta_k, \\theta) = \\arg \\max_\\theta \\underset{s,a\\sim\\pi_{\\theta_k}}{\\mathbb{E}} \\left[L(s, a, \\theta_k, \\theta)\\right]$$ typically taking multiple steps of SGD to maximize the objective. Here $L$ is given by:\n$$L(s, a, \\theta_k, \\theta) = \\min\\Bigg(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A_{\\pi_{\\theta_k}}(s,a), \\text{clip}\\left(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}, 1-\\epsilon, 1+\\epsilon\\right)A_{\\pi_{\\theta_k}}(s,a)\\Bigg)$$ PPO-Clip doesn\u0026rsquo;t have a KL-divergence term in the objective and doesn\u0026rsquo;t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\nGeneralized Advantage Estimation (GAE) is used in PPO to estimate the advantage function:\n$$A_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} \\\\ = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} + \\dots$$ where:\n$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ (TD error)\nthe reward $r_t$ is usually given by a trained (neural) reward model $\\gamma$ is the discount factor\n$\\lambda$ is the GAE parameter (tradeoff between bias and variance)\n$V(s)$ is the value function estimate, from value network\nOften a value network $V_\\psi$ needs to be trained alongside the policy model $\\pi_\\theta$ to estimate the value, often as big as the policy model itself General RL Training Setup Objective: $$J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$ where $\\tau = \\{s_0, a_0, r_1, s_1, a_1, \\dots, s_T\\}$ is a trajectory. For LLMs, actions $a_t$ here are tokens!\nPolicy Gradient: $$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{s,a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\, Q^{\\pi_\\theta}(s,a)\\right]$$ or equivalently, using the advantage function $A^{\\pi_\\theta}(s,a)$: $$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{s,a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\, A^{\\pi_\\theta}(s,a)\\right]$$ Trajectory Collection: LLM interacts with the environment (reward model) to sample trajectories. For each timestep: $$s_t \\xrightarrow{\\pi_\\theta} a_t \\rightarrow r_{t+1},\\, s_{t+1}$$ Return and Advantage Calculation: $$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}, \\quad A(s_t,a_t) = Q^{\\pi_\\theta}(s_t,a_t) - V^{\\pi_\\theta}(s_t)$$ Generalized Advantage Estimation (GAE) are used to compute $A(s_t,a_t)$ practically\nPolicy Update: $$\\theta_{k+1} = \\theta_k + \\alpha\\, \\nabla_\\theta J(\\pi_\\theta)$$ Using the computed advantages, update the policy parameters and value network by maximizing a surrogate objective (e.g., the PPO objective) to favor actions with higher expected returns. Repeat until agent\u0026rsquo;s performance converges.\nR1-Zero Group Relative Policy Optimization (GRPO) DeepSeek uses GRPO (created in-house) which foregoes the value network and estimates the advantages directly from group rewards instead.\nFor each question $q$, GRPO samples a group of outputs $\\{o_i\\}_{i=1}^G$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing:\n$$\\begin{aligned} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(Q|q)] \\\\ \u0026\\quad \\cdot \\frac{1}{G}\\sum_{i=1}^G\\left(\\min\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}A_i, \\text{clip}\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon\\right)A_i\\right) - \\beta\\mathbb{D}_{KL}(\\pi_\\theta\\|\\pi_{\\text{ref}})\\right) \\end{aligned}$$ $$\\mathbb{D}_{KL}(\\pi_\\theta\\|\\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - \\log\\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - 1$$ where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_i$ is the advantage 1\nGRPO Advantages\nAdvantages $A_i$ are computed using group rewards $\\{r_1,r_2,\\ldots,r_G\\}$:\n$$A_i = \\frac{r_i - \\text{mean}(\\{r_1,r_2,\\ldots,r_G\\})}{\\text{std}(\\{r_1,r_2,\\ldots,r_G\\})} $$ As the value network is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. GRPO eliminates value network completely, saving enourmous amounts of compute!\nReward Modelling\nR1-Zero uses a rule-based reward system consisting of:\nAccuracy rewards: The accuracy reward model evaluates whether the response is correct. For math problems with deterministic results, can easily check if right or wrong. Provider higher reward if right. For LeetCode problems, a compiler can be used to generate feedback based on predefined test cases Format rewards: Enforces the model to put its thinking process between \u0026lt;think\u0026gt; and \u0026lt;/think\u0026gt; tags. Usually for LLMs, a neural reward model (neural network used to estimate rewards) is used to provide rewards, as human preferences or requirements are difficult to define in a rules-based way. However, R1 does not use any neural reward model:\nNeural reward model may suffer from reward hacking\nRetraining the reward model needs additional training resources and it complicates the whole training pipeline\nPerformance\nThere\u0026rsquo;s no supervised fine tuning involved at all in R1-Zero, which is extremely impressive considering its performance. Majority voting bring performance on AIME benchmark from 71% to 86%.\nR1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation (increased Chain-Of-Thought length). Behaviours like backtracking and reflection naturally emerge, which is absolutely incredible\u0026hellip;\nProblems\nR1-Zero exhibits poor readability of its chain of thoughts, where language mixing often occurs. I suspect this is probably some form of reward over-optimization, as it is more efficient to reason in its own language and achieve higher reward.\nWe know different languages have different information density. This can be thought of as the \u0026ldquo;efficiency\u0026rdquo; of language, given a constraint on its length. Suppose an LLM had a length constraint (the context window), and was incentivised to achieve a goal. Won\u0026rsquo;t you expect the LLM to naturally use more information dense languages? Or even more extreme - if it were intelligent enough - create its own language that is information dense enough to achieve its goals?\nI worry about this aspect quite abit, about future intelligent systems reasoning in its own language uninterpretable to humans. If you\u0026rsquo;ve ever seen Villeneuve\u0026rsquo;s incredible film Arrival, you would have seen the heptapods\u0026rsquo; non-linear language which isn\u0026rsquo;t formed by sequentially combining words linearly. If these systems start communicating in their own language and are given goals by humans, you can see how this could cause all sorts of various problems.\nR1 Training Pipeline Phase 1: Cold Start Construct and collect small amounts (thousands) of long CoT data to fine-tune V3 Using few-shot prompting on V3 with long CoT examples Direct prompting on V3 for detailed answers with reflection Gathering DeepSeek-R1-Zero outputs with human post-processing Key advantages: Readability: R1-Zero has unreadable responses. We define output format as |special_token|\u0026lt;reasoning_process\u0026gt;|special_token|\u0026lt;summary\u0026gt; so reasoning process (CoT) and summary of reasoning provided Potential: Carefully designed patterns with human priors show better performance vs R1-Zero Phase 2: Reasoning Reinforcement Learning After cold start fine-tuning, apply large-scale RL training similar to DeepSeek-R1-Zero:\nFocus on reasoning-intensive tasks: Coding, mathematics, science, logic reasoning Well-defined problems with clear solutions Language Consistency: CoT exhibits language mixing in multi-language prompts Introduce language consistency reward, measured as proportion of target language words in CoT Final reward function combines reasoning task accuracy, language consistency reward, and formatting reward to train until convergence.\nPhase 3: Rejection Sampling and Supervised Fine-Tuning After RL convergence, collect SFT data from checkpoint for further training. Data includes:\nReasoning Data (600k samples)\nRejection sampling from RL checkpoint Expanded dataset includes: Rule-based rewards Generative rewards via DeepSeek-V3 as LLM-Judge Quality filters: Remove mixed languages Filter long paragraphs Remove chaotic code blocks Non-Reasoning Data (200k samples)\nTypes: Writing Factual QA Self-cognition Translation Reuse SFT data for DeepSeek-V3 Final fine-tuning on DeepSeek-V3-Base: 2 epochs on combined 800k samples\nPhase 4: Diverse Reinforcement Learning Phase Secondary RL stage to align with human preferences while improving, helpfulnes, harmlessness, and reasoning:\nTraining Approach\nReasoning data: Same approach as DeepSeek-R1-Zero Rule-based rewards on Math, code, logical reasoning General data: Reward model as in DeepSeek V3 Use preference pairs Helpful Honest Harmless (HHH) Criteria\nHelpfulness: Focus on final summary in summary tag is useful and relevance Preserves reasoning process in the reasoning tag Harmlessness: Evaluate full response for potential biases or hamful content R1 Engineering Unlocks The DeepSeek team also did some insane software and hardware optimizations:\nMulti-Head Latent Attention (for efficient inference) Low-rank join compression for K,Q,V matrices to reduce key-value cache memory usage DeepSeekMoE with Auxiliary-Loss-Free Load Balancing (efficient training) 671B parameters, but only which 37B are activated for each token Multi-Token Prediction Objective Conclusion I think the core reason behind R1\u0026rsquo;s success was its sheer efficiency: getting rid of the value network via simple advantage estimation, using a simple reward model instead of a neural reward model, and all the hardware/software optimizations. The multi-phase training approach, while quite hacky, preserves the reasoning ability of R1-Zero while ensuring readability. All in all I\u0026rsquo;m very impressed by the DeepSeek team, and I hope you learnt something from this post!\nReferences Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., \u0026amp; Guo, D. (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300 [cs.CL]. https://arxiv.org/abs/2402.03300\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., et al. (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL]. https://arxiv.org/abs/2501.12948\nDeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., et al. (2025). DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL]. https://arxiv.org/abs/2412.19437\nARENA curriculum. Transformer from Scratch. https://arena-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch\nInstead of adding KL penalty in the reward like in PPO, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of advantage $A$\u0026#160;\u0026#x21a9;\u0026#xfe0e;","href":"https://liuzihe02.github.io/posts/r1/","kind":"page","lang":"en","lastmod":"2025-02-17T17:05:08Z","objectID":"aea9ae4ea09924bdadd709902f9af108","publishDate":"2025-02-17T17:05:08Z","section":"posts","tags":[],"title":"Breaking Down DeepSeek R1","type":"posts"},{"content":"We explain how Monte Carlo Tree Search works and introduce its pseudocode. We borrow notes and figures from int8\u0026rsquo;s website and the MCTS Wikipedia page. I wrote a lightweight implementation for tic-tac-toe available here and a super lightweight implementation available here.\nGame Representation A game tree is a tree data structure, where every node represents a distinct state of the game. Given a state $s$, we take an action $a$, and the environment transitions to the new state $s'$ with the transition probability $p_a(s'|s)$. If the transition probablity is $1$ (such as in simple deterministic games like Tic Tac Toe), then each node in the game tree represents a state.\nWe use \u0026ldquo;action\u0026rdquo; and \u0026ldquo;move\u0026rdquo; interchangeably in the context of games\nThe root of the game tree represents the initial state of the game Any transition from one state to another consists a move The game ends at a terminal node Monte Carlo Tree Search Taken from CadiaPlayer Our game tree contains various connected game states, with each node in the tree containing statistics used to compute the value of a particular state. During \u0026ldquo;training\u0026rdquo;, MCTS traverses down the game tree from the root node $R$ until a leaf node $L$ is reached, then expands the game tree by adding a child node $C$.\nAfter expanding the game tree, we simulate the game from the child node until game termination at $T$ - a rollout. The results of the rollout is then used to update the game tree, so that we can choose better nodes to do rollouts from next time.\nThis way we make the game tree contain better estimates of the values of each states, which allows us to make better actions when we actually play the game. This occurs through the 4 stages: selection, expansion, simulation, and backpropagation.\nSelection The bold circles contain the nodes selected using the tree policy UCT. The color of the node represent whose turn it is to move next. The numbers in the nodes represent the statistics of the node. For example, a black circle with $7/10$ means black to move next, $7$ wins played from this state, total of $10$ visits at this state. Hence if there are no draws, white would have won $3$ times from this node.\nThe first phase, selection, works by starting at the root $R$ (initial state of game) and traversing down the game tree using the tree policy until a leaf node $L$. The tree policy used here is Upper Confidence Trees (UCT), which is UCB1 applied to trees:\n$$ a = \\arg\\max_{a \\in A(s)} \\left\\{ Q(s,a) + C_p\\sqrt{\\frac{\\ln N(s)}{N(s,a)}} \\right\\} $$ where $N(s)$ is the number of times a node has been visited, $N(s,a)$ is the number of times $a$ has been selected from this node, and $C_p$ is a exploration constant. Increasing $C_p$ will encourage more exploration, while decreasing it encourages a more greedy approach, but $\\sqrt2$ is often chosen.\nIf the transition probability is 1 - taking an action will deterministically move the state to another - then UCT simplifies to\n$$ UCT(v) = \\arg\\max_{i} \\left\\{ \\frac{w(v_i)}{N(v_i)} + C_p \\sqrt{\\frac{\\ln N(v) }{N(v_i)}} \\right\\} $$ for a parent node $v$, child node $v_i$, and number of wins $w$ from this child node $v_i$. UCT It is a sum of two components – the first component of our function $ \\frac{w(v_i)}{N(v_i)} $, also called exploitation component, can be read as a win rate. The term on the right is the exploration component - encouraging child nodes that have little visits. During selection, we want to allow some exploration, hence $C_p\u003e0$ here.\nA leaf node $L$ here is a node in the game tree, where not all actions have been explored. In other words, it still has unexplored game states (if one action transitions to a state deterministically). Once a leaf node has no more unexplored actions, it is fully expanded.\nExpansion $3/3$ still has unexplored actions, hence we add a child node to it. This action then becomes \u0026ldquo;explored\u0026rdquo;\nThe second phase expansion occurs when we\u0026rsquo;ve selected a leaf node $L$ to expand on. An unexplored node is randomly chosen, and is added to the game tree as a child node.\nSimulation Once the new child is added to the tree, we do one rollout from the $0/0$ until game termination\nWe do one simulation (rollout) from the new child node $C$ until game termination, where moves are chosen according to the rollout policy. This rollout policy is often completely random.\nBackpropagation We can see only black nodes have their win count updated.\nAfter the simulation reaches an end, all of the nodes taken in this path are updated. All of the nodes number of visits are incremented by one each, and the colour that matches the winner will have its win count incremented by 1. Hence useful statistics to keep are the number of visits and win count for that player.\nIn this repo, we store the win counts of both players in each node. A node also stores the board state, which contains which player it is to move next\nPseudoCode def select(node): while node is not terminal: if node is not fully expanded (has unexplored actions): return node else: node = UCT(node) return node def expand(node): assert unexplored_actions\u0026gt;0 action = unexplored_actions.pop() add this action as a child (to the game tree) return child def simulate(node): while state is not terminal choose an action (rollout policy) move the state return the result of game def backpropagate(node,result): update this node if parent exists: backpropagate(parent,result) def UCT(node,c): choices = (child.value/child.visits)+ c*np.sqrt((np.log(parent.visits)/child.visits)) return node.children[argmax(choices)] def train(root): leaf=select(root) if leaf has unexplored actions: child=expand(leaf) reward=simulate(child) backpropagate(child,reward) else: reward=simulate(leaf) backpropagate(leaf, reward)","href":"https://liuzihe02.github.io/posts/mcts/","kind":"page","lang":"en","lastmod":"2024-12-07T17:05:08Z","objectID":"4866ff99ed4a5a615c61a11335a5d2e8","publishDate":"2024-12-07T17:05:08Z","section":"posts","tags":[],"title":"Breaking Down Monte Carlo Tree Search","type":"posts"}]